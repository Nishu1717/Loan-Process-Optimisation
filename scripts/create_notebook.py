import json
import os

notebook_content = {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Process Optimisation Project\n",
    "\n",
    "This notebook covers Phase 1 (Data Hardening), Phase 2 (Sanitization & Bottleneck Analysis), Phase 3 (Transition Analysis), Phase 4 (To-Be Process), and Phase 5 (Load Balancing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define file paths\n",
    "DATA_DIR = \"../data\"\n",
    "CLEANED_FILE = os.path.join(DATA_DIR, \"bpi_2017_cleaned.csv\")\n",
    "HARDENED_FILE = os.path.join(DATA_DIR, \"bpi_2017_hardened.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Data Hardening\n",
    "\n",
    "**Goal**: Fill missing values for case-level and offer-level attributes to create a \"hardened\" dataset.\n",
    "**Output**: `bpi_2017_hardened.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- Starting Phase 1 ---\")\n",
    "print(f\"Loading raw data from {CLEANED_FILE}...\")\n",
    "df_phase1 = pd.read_csv(CLEANED_FILE)\n",
    "\n",
    "case_attributes = ['CreditScore', 'case:RequestedAmount', 'MonthlyCost']\n",
    "case_id_col = 'case:concept:name'\n",
    "\n",
    "print(\"Filling case-level attributes (CreditScore, RequestedAmount, MonthlyCost)...\")\n",
    "for col in case_attributes:\n",
    "    df_phase1[col] = df_phase1.groupby(case_id_col)[col].transform(lambda x: x.ffill().bfill())\n",
    "\n",
    "offer_attributes = ['OfferedAmount']\n",
    "offer_id_col = 'OfferID'\n",
    "\n",
    "print(\"Filling offer-level attributes (OfferedAmount)...\")\n",
    "# Use dropna=False to ensure we process groups even if grouping keys have NaNs (though we care about OfferID)\n",
    "for col in offer_attributes:\n",
    "    df_phase1[col] = df_phase1.groupby([case_id_col, offer_id_col], dropna=False)[col].transform(lambda x: x.ffill().bfill())\n",
    "\n",
    "print(f\"Saving hardened data to {HARDENED_FILE}...\")\n",
    "df_phase1.to_csv(HARDENED_FILE, index=False)\n",
    "print(\"Phase 1 complete. Hardened file saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Sanitization and Bottleneck Analysis\n",
    "\n",
    "**Goal**: Sanitize data (fix missing offers, mixed credit scores) and calculate process metrics.\n",
    "**Input**: `bpi_2017_hardened.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- Starting Phase 2 ---\")\n",
    "print(f\"Loading hardened data from {HARDENED_FILE}...\")\n",
    "df = pd.read_csv(HARDENED_FILE)\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "if 'time:timestamp' in df.columns:\n",
    "    df['time:timestamp'] = pd.to_datetime(df['time:timestamp'], format='mixed', utc=True)\n",
    "\n",
    "print(f\"Loaded {len(df)} rows for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1: Data Sanitization\n",
    "\n",
    "- Fix missing `OfferedAmount` by unifying `EventID` (from O_Create Offer) and `OfferID`.\n",
    "- Create `is_viable_offer` flag.\n",
    "- Handle mixed `CreditScore` (Initial vs Final)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Handle Missing OfferedAmount\n",
    "print(\"Unifying Offer IDs and filling OfferedAmount...\")\n",
    "df['TempOfferID'] = df['OfferID']\n",
    "# For O_Create Offer, the OfferID is actually in the EventID column\n",
    "mask_create_offer = df['concept:name'] == 'O_Create Offer'\n",
    "df.loc[mask_create_offer, 'TempOfferID'] = df.loc[mask_create_offer, 'EventID']\n",
    "\n",
    "# Now fill OfferedAmount by grouping by TempOfferID\n",
    "df['OfferedAmount'] = df.groupby('TempOfferID')['OfferedAmount'].transform(lambda x: x.ffill().bfill())\n",
    "\n",
    "# Create is_viable_offer\n",
    "# True if TempOfferID is present AND OfferedAmount is populated. \n",
    "df['is_viable_offer'] = (df['TempOfferID'].notna() & df['OfferedAmount'].notna())\n",
    "print(f\"Created 'is_viable_offer'. Count of viable offers: {df['is_viable_offer'].sum()}\")\n",
    "\n",
    "# 2. Handle Mixed Credit Scores\n",
    "print(\"Handling mixed CreditScores...\")\n",
    "df.sort_values(by=['case:concept:name', 'time:timestamp'], inplace=True)\n",
    "\n",
    "credit_score_groups = df.groupby('case:concept:name')['CreditScore']\n",
    "df['initial_credit_score'] = credit_score_groups.transform('first')\n",
    "df['final_credit_score'] = credit_score_groups.transform('last')\n",
    "\n",
    "print(\"CreditScore columns created (initial_credit_score, final_credit_score).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2: The Bottleneck Engine\n",
    "\n",
    "- Calculate `Time-to-Offer`.\n",
    "- Calculate `Process Efficiency Ratio`.\n",
    "- Generate `Cycle Time Histogram` (14-Day Validation).\n",
    "- Generate `Wait Time Heatmap` (Queue Time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Time-to-Offer\n",
    "print(\"Calculating Time-to-Offer...\")\n",
    "case_starts = df[df['concept:name'] == 'A_Create Application'].groupby('case:concept:name')['time:timestamp'].min()\n",
    "first_offers = df[df['concept:name'] == 'O_Create Offer'].groupby('case:concept:name')['time:timestamp'].min()\n",
    "\n",
    "time_to_offer = (first_offers - case_starts).dt.total_seconds() / 3600.0 # Hours\n",
    "\n",
    "print(\"Time-to-Offer (Hours) Stats:\")\n",
    "print(time_to_offer.describe())\n",
    "\n",
    "# 2. Process Efficiency Ratio\n",
    "print(\"Calculating Process Efficiency Ratio...\")\n",
    "# Queue Time Heuristic: Match 'schedule' to subsequent 'start' by Activity\n",
    "transitions_df = df[df['lifecycle:transition'].isin(['schedule', 'start'])].copy()\n",
    "transitions_df.sort_values(['case:concept:name', 'concept:name', 'time:timestamp'], inplace=True)\n",
    "\n",
    "# Shift logic for queue time\n",
    "transitions_df['next_transition'] = transitions_df['lifecycle:transition'].shift(-1)\n",
    "transitions_df['next_timestamp'] = transitions_df['time:timestamp'].shift(-1)\n",
    "transitions_df['next_case'] = transitions_df['case:concept:name'].shift(-1)\n",
    "transitions_df['next_activity'] = transitions_df['concept:name'].shift(-1)\n",
    "transitions_df['next_resource'] = transitions_df['org:resource'].shift(-1) \n",
    "\n",
    "valid_pair = (\n",
    "    (transitions_df['lifecycle:transition'] == 'schedule') & \n",
    "    (transitions_df['next_transition'] == 'start') & \n",
    "    (transitions_df['case:concept:name'] == transitions_df['next_case']) & \n",
    "    (transitions_df['concept:name'] == transitions_df['next_activity'])\n",
    ")\n",
    "\n",
    "transitions_df.loc[valid_pair, 'queue_time'] = (transitions_df['next_timestamp'] - transitions_df['time:timestamp']).dt.total_seconds()\n",
    "transitions_df.loc[valid_pair, 'resource_picker'] = transitions_df['next_resource']\n",
    "\n",
    "# Active time: start -> complete\n",
    "active_df = df[df['lifecycle:transition'].isin(['start', 'complete'])].copy()\n",
    "active_df.sort_values(['case:concept:name', 'concept:name', 'time:timestamp'], inplace=True)\n",
    "\n",
    "active_df['next_transition'] = active_df['lifecycle:transition'].shift(-1)\n",
    "active_df['next_timestamp'] = active_df['time:timestamp'].shift(-1)\n",
    "active_df['next_case'] = active_df['case:concept:name'].shift(-1)\n",
    "active_df['next_activity'] = active_df['concept:name'].shift(-1)\n",
    "\n",
    "valid_active_pair = (\n",
    "    (active_df['lifecycle:transition'] == 'start') & \n",
    "    (active_df['next_transition'] == 'complete') & \n",
    "    (active_df['case:concept:name'] == active_df['next_case']) & \n",
    "    (active_df['concept:name'] == active_df['next_activity'])\n",
    ")\n",
    "\n",
    "active_df.loc[valid_active_pair, 'active_duration'] = (active_df['next_timestamp'] - active_df['time:timestamp']).dt.total_seconds()\n",
    "total_active_time = active_df.groupby('case:concept:name')['active_duration'].sum()\n",
    "\n",
    "case_end_times = df.groupby('case:concept:name')['time:timestamp'].max()\n",
    "case_start_times = df.groupby('case:concept:name')['time:timestamp'].min()\n",
    "total_cycle_time = (case_end_times - case_start_times).dt.total_seconds()\n",
    "\n",
    "efficiency_ratio = total_active_time / total_cycle_time\n",
    "print(\"Process Efficiency Ratio dataframe head:\")\n",
    "print(efficiency_ratio.head())\n",
    "\n",
    "# 3. 14-Day Validation (Cycle Time Histogram)\n",
    "print(\"Generating Cycle Time Histogram (14-Day Validation)...\")\n",
    "cycle_time_days = total_cycle_time / (24 * 3600)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(cycle_time_days, bins=50, edgecolor='k')\n",
    "plt.title('Distribution of Case Cycle Times (Days)')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(14, color='r', linestyle='--', label='14 Days')\n",
    "plt.legend()\n",
    "plt.savefig('../visualizations/cycle_time_histogram.png')\n",
    "print(\"Histogram saved to cycle_time_histogram.png\")\n",
    "\n",
    "# 4. Wait Time Heatmap (Queue Time)\n",
    "print(\"Calculating Queue Time Heatmap...\")\n",
    "queue_data = transitions_df[valid_pair][['queue_time', 'resource_picker']]\n",
    "if not queue_data.empty:\n",
    "    avg_queue_time = queue_data.groupby('resource_picker')['queue_time'].mean().sort_values(ascending=False)\n",
    "    print(\"Top 5 Users with Highest Queue Time (Seconds):\")\n",
    "    print(avg_queue_time.head())\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_50 = avg_queue_time.head(50)\n",
    "    plt.barh(top_50.index.astype(str), top_50.values)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title('Average Queue Time by User (Top 50)')\n",
    "    plt.xlabel('Seconds')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../visualizations/wait_time_heatmap.png')\n",
    "    print(\"Heatmap saved to wait_time_heatmap.png\")\n",
    "else:\n",
    "    print(\"No valid queue time pairs found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Transition Analysis & Prediction\n",
    "\n",
    "## Step 3.1: State Transition Analysis\n",
    "- Identify longest idle times between transitions.\n",
    "- Segment transitions by `LoanGoal`.\n",
    "- Analyze Ping-Pong effect (W_Validate application)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- Starting Phase 3 --- \")\n",
    "\n",
    "# 1. Calculate Idle Time for Transitions\n",
    "print(\"Calculating State Transitions and Idle Times...\")\n",
    "df.sort_values(by=['case:concept:name', 'time:timestamp'], inplace=True)\n",
    "\n",
    "# Shift to get next activity and time\n",
    "df['next_activity'] = df.groupby('case:concept:name')['concept:name'].shift(-1)\n",
    "df['next_timestamp'] = df.groupby('case:concept:name')['time:timestamp'].shift(-1)\n",
    "\n",
    "# Filter out last events (where next_activity is NaN)\n",
    "transitions = df.dropna(subset=['next_activity']).copy()\n",
    "\n",
    "# Calculate duration to next event (proxy for idle/transition time)\n",
    "transitions['transition_duration'] = (transitions['next_timestamp'] - transitions['time:timestamp']).dt.total_seconds()\n",
    "transitions['transition_pair'] = transitions['concept:name'] + \" -> \" + transitions['next_activity']\n",
    "\n",
    "# Top 10 bottleneck transitions\n",
    "avg_transition_time = transitions.groupby('transition_pair')['transition_duration'].mean().sort_values(ascending=False)\n",
    "print(\"Top 10 Slowest Transition Pairs (Avg Seconds):\")\n",
    "print(avg_transition_time.head(10))\n",
    "\n",
    "# 2. Segment by LoanGoal\n",
    "if 'case:LoanGoal' in df.columns:\n",
    "    print(\"\\nSegmenting Bootlenecks by LoanGoal...\")\n",
    "    # Get top 1 bottleneck\n",
    "    top_bottleneck = avg_transition_time.index[0]\n",
    "    print(f\"Deep Dive into Top Bottleneck: {top_bottleneck}\")\n",
    "    \n",
    "    subset = transitions[transitions['transition_pair'] == top_bottleneck]\n",
    "    goal_stats = subset.groupby('case:LoanGoal')['transition_duration'].agg(['mean', 'count']).sort_values(by='mean', ascending=False)\n",
    "    print(goal_stats)\n",
    "else:\n",
    "    print(\"LoanGoal column not found.\")\n",
    "\n",
    "# 3. Ping-Pong Analysis (W_Validate application)\n",
    "print(\"\\nCalculating Ping-Pong Counts for 'W_Validate application'...\")\n",
    "validate_counts = df[df['concept:name'] == 'W_Validate application'].groupby('case:concept:name').size()\n",
    "validate_counts.name = 'validation_count'\n",
    "\n",
    "# Prepare Cycle Time data again for merge (using Days)\n",
    "cycle_time_df = pd.DataFrame(cycle_time_days)\n",
    "cycle_time_df.columns = ['cycle_time_days']\n",
    "\n",
    "# Merge\n",
    "pingpong_df = cycle_time_df.join(validate_counts, how='left').fillna(0)\n",
    "\n",
    "# Scatter Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pingpong_df['validation_count'], pingpong_df['cycle_time_days'], alpha=0.5)\n",
    "plt.title('Ping-Pong Effect: Validation Count vs Cycle Time')\n",
    "plt.xlabel('Number of \"W_Validate application\" Events')\n",
    "plt.ylabel('Total Cycle Time (Days)')\n",
    "plt.savefig('../visualizations/validation_pingpong.png')\n",
    "print(\"Scatter plot saved to validation_pingpong.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: The \"Happy Path\" vs \"Clogged Path\"\n",
    "\n",
    "- Define Group A (Fast) and Group B (Laggards).\n",
    "- Compare features (RequestedAmount, CreditScore) to find drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Groups\n",
    "print(\"Defining Groups A (Fast) and B (Laggards)...\")\n",
    "group_a = pingpong_df[pingpong_df['cycle_time_days'] <= 5].index\n",
    "group_b = pingpong_df[pingpong_df['cycle_time_days'] >= 20].index\n",
    "\n",
    "print(f\"Group A (<= 5 days) count: {len(group_a)}\")\n",
    "print(f\"Group B (>= 20 days) count: {len(group_b)}\")\n",
    "\n",
    "# 2. Feature Correlation / Comparison\n",
    "# We need case-level attributes. Create a case-level dataframe.\n",
    "# Taking 'first' for attributes that shouldn't change or we want initial value\n",
    "case_features = df.groupby('case:concept:name').agg({\n",
    "    'case:RequestedAmount': 'first',\n",
    "    'initial_credit_score': 'first',\n",
    "    'case:LoanGoal': 'first'\n",
    "})\n",
    "\n",
    "# Flag the groups\n",
    "case_features['is_group_b'] = 0\n",
    "case_features.loc[group_b, 'is_group_b'] = 1\n",
    "# Filter to only A and B for comparison\n",
    "analysis_df = case_features.loc[group_a.union(group_b)].copy()\n",
    "\n",
    "print(\"\\nComparing Features between Fast (A) vs Slow (B)...\")\n",
    "print(\"Mean Values:\")\n",
    "print(analysis_df.groupby('is_group_b')[['case:RequestedAmount', 'initial_credit_score']].mean())\n",
    "\n",
    "# Correlation in the combined set\n",
    "corr_amount = analysis_df['is_group_b'].corr(analysis_df['case:RequestedAmount'])\n",
    "corr_score = analysis_df['is_group_b'].corr(analysis_df['initial_credit_score'])\n",
    "\n",
    "print(f\"\\nCorrelation with being in Group B (Slow):\")\n",
    "print(f\"RequestedAmount: {corr_amount:.4f}\")\n",
    "print(f\"CreditScore: {corr_score:.4f}\")\n",
    "\n",
    "if 'case:LoanGoal' in analysis_df.columns:\n",
    "    print(\"\\nLoanGoal Distribution in Group B (Slow):\")\n",
    "    print(analysis_df[analysis_df['is_group_b'] == 1]['case:LoanGoal'].value_counts(normalize=True).head())\n",
    "    print(\"\\nLoanGoal Distribution in Group A (Fast):\")\n",
    "    print(analysis_df[analysis_df['is_group_b'] == 0]['case:LoanGoal'].value_counts(normalize=True).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: The Strategic \"To-Be\" Process Map\n",
    "\n",
    "## Step 4.1: BPMN 2.0 Logic for AI-OCR Document Validator\n",
    "\n",
    "**Objective**: Replace the manual \"Incomplete File\" loop with an automated triage system.\n",
    "\n",
    "**To-Be Flow Idea**:\n",
    "1. **Trigger**: `A_Submitted` event occurs (Customer submits document).\n",
    "2. **Action**: AI-OCR Service (e.g., Google Document AI) instantly scans documents.\n",
    "3. **Decision Gateway**: \n",
    "    - **If Incomplete/Missing**: System triggers `Send_Automated_SMS_Email` -> Loop back to customer. (Bypasses human queue).\n",
    "    - **If Complete**: System routes case to `W_Validate application` queue.\n",
    "\n",
    "## Step 4.2: Theoretical Impact Calculation\n",
    "**Assumption**: We can reduce the delay of `W_Call incomplete files` -> `W_Personal Loan collection` by **90%** via automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- Starting Phase 4: To-Be Process & Impact ---\")\n",
    "\n",
    "# 1. Calculate Current Cost of Incomplete Files\n",
    "target_transition = \"W_Call incomplete files -> W_Personal Loan collection\"\n",
    "\n",
    "if 'transition_duration' in transitions.columns:\n",
    "    incomplete_file_transition = transitions[transitions['transition_pair'] == target_transition]\n",
    "    total_delay_seconds = incomplete_file_transition['transition_duration'].sum()\n",
    "    avg_delay_seconds = incomplete_file_transition['transition_duration'].mean()\n",
    "    count_occurrences = len(incomplete_file_transition)\n",
    "    \n",
    "    print(f\"Current State Analysis for '{target_transition}':\")\n",
    "    print(f\"  - Occurrences: {count_occurrences}\")\n",
    "    print(f\"  - Total Delay Time: {total_delay_seconds:,.0f} seconds ({total_delay_seconds/3600:,.0f} hours)\")\n",
    "    print(f\"  - Average Delay per Case: {avg_delay_seconds:,.0f} seconds ({avg_delay_seconds/3600:.2f} hours)\")\n",
    "    \n",
    "    # 2. Calculate Theoretical Savings (90% Reduction)\n",
    "    theoretical_reduction_factor = 0.90\n",
    "    time_saved_seconds = total_delay_seconds * theoretical_reduction_factor\n",
    "    \n",
    "    print(f\"\\nTheoretical 'To-Be' Impact (90% Reduction via AI-OCR):\")\n",
    "    print(f\"  - Potential Time Saved: {time_saved_seconds:,.0f} seconds\")\n",
    "    print(f\"  - Potential Time Saved (Hours): {time_saved_seconds/3600:,.0f} hours\")\n",
    "    \n",
    "else:\n",
    "    print(\"Transition data not available from previous steps. Please re-run Phase 3 logic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5: Resource Load Balancing Algorithm\n",
    "\n",
    "## Step 5.1: Identify Laggard Users\n",
    "Using the queue time data from Phase 2, identify the top 10% users with the highest average queue times.\n",
    "\n",
    "## Step 5.2: Dynamic Resource Allocator Simulation\n",
    "Redistribute tasks from 'Laggards' to 'Efficient Users' (Bottom 50% queue time). Calculate the new theoretical total queue time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- Starting Phase 5: Resource Load Balancing ---\")\n",
    "\n",
    "# 1. Identify Laggards (Top 10%)\n",
    "# We use avg_queue_time from Phase 2\n",
    "if 'avg_queue_time' in locals() and not avg_queue_time.empty:\n",
    "    n_users = len(avg_queue_time)\n",
    "    n_laggards = int(np.ceil(n_users * 0.10)) # Top 10%\n",
    "    \n",
    "    laggards = avg_queue_time.head(n_laggards).index\n",
    "    efficient_users = avg_queue_time.tail(int(n_users * 0.50)).index # Bottom 50% (Fastest) aka lowest queue time\n",
    "    \n",
    "    print(f\"Total Users with Queue Data: {n_users}\")\n",
    "    print(f\"Identified {len(laggards)} Laggard Users (Top 10%).\")\n",
    "    print(f\"Identified {len(efficient_users)} Efficient Users (Bottom 50%).\")\n",
    "    \n",
    "    avg_efficient_queue_time = avg_queue_time[efficient_users].mean()\n",
    "    print(f\"Average Queue Time of Efficient Users: {avg_efficient_queue_time:.2f} seconds\")\n",
    "    \n",
    "    # 2. Python Simulation\n",
    "    print(\"Running Dynamic Resource Allocator Simulation...\")\n",
    "    \n",
    "    # Work on the subset of transitions that have queue times\n",
    "    simulation_df = transitions_df[transitions_df['queue_time'].notna()].copy()\n",
    "    \n",
    "    # Calculate As-Is Total\n",
    "    total_queue_as_is = simulation_df['queue_time'].sum()\n",
    "    \n",
    "    # Identify tasks assigned to laggards\n",
    "    laggard_tasks_mask = simulation_df['resource_picker'].isin(laggards)\n",
    "    count_laggard_tasks = laggard_tasks_mask.sum()\n",
    "    \n",
    "    print(f\"Tasks handled by Laggards: {count_laggard_tasks}\")\n",
    "    \n",
    "    # Redistribute: Assign these tasks the average queue time of efficient users\n",
    "    # (Simulating instant routing to a better resource)\n",
    "    simulation_df.loc[laggard_tasks_mask, 'simulated_queue_time'] = avg_efficient_queue_time\n",
    "    # Keep others as is\n",
    "    simulation_df.loc[~laggard_tasks_mask, 'simulated_queue_time'] = simulation_df.loc[~laggard_tasks_mask, 'queue_time']\n",
    "    \n",
    "    total_queue_simulated = simulation_df['simulated_queue_time'].sum()\n",
    "    \n",
    "    # 3. Gap Analysis\n",
    "    print(f\"\\nGap Analysis (As-Is vs Simulated):\")\n",
    "    print(f\"  - Total Queue Time (As-Is): {total_queue_as_is:,.0f} seconds\")\n",
    "    print(f\"  - Total Queue Time (Simulated): {total_queue_simulated:,.0f} seconds\")\n",
    "    \n",
    "    improvement = total_queue_as_is - total_queue_simulated\n",
    "    pct_improvement = (improvement / total_queue_as_is) * 100\n",
    "    \n",
    "    print(f\"  - Net Improvement: {improvement:,.0f} seconds\")\n",
    "    print(f\"  - % Reduction in Queue Time: {pct_improvement:.2f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"Avg Queue Time data missing. Cannot run Phase 5.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}


# Determine the directory where this script is located
script_dir = os.path.dirname(os.path.abspath(__file__))
# Construct the path to the notebooks directory (sibling to scripts)
output_path = os.path.join(script_dir, "../notebooks/Loan_Process_Optimisation.ipynb")

with open(output_path, "w") as f:
    json.dump(notebook_content, f, indent=1)
